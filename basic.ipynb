{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53124c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f5b2f",
   "metadata": {},
   "source": [
    "# Task 1: Left-Right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "38dee3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "C:\\Users\\Aniket Konkar\\AppData\\Local\\Temp\\ipykernel_15128\\2408679250.py:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "  data = np.load('data\\LR_task_with_antisaccade_synchronised_min.npz')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30842, 2)\n",
      "Converted to\n",
      "(30842,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data\\LR_task_with_antisaccade_synchronised_min.npz')\n",
    "print(data['labels'].shape)\n",
    "print('Converted to')\n",
    "print(data['labels'][:, 1].shape)\n",
    "\n",
    "trainX = data['EEG']\n",
    "trainY = data['labels'][:, 1]\n",
    "ids = data['labels'][:, 0] # Participant Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fedd2e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30842, 500, 129)\n",
      "(30842,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be24320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=129, num_layers=2, num_heads=3, dim_feedforward=512):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True  # Input shape: [batch, seq, dim]\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head: input_dim -> 1 (binary classification)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # For binary output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)  # Shape: [batch, seq_len, input_dim]\n",
    "        pooled = encoded.mean(dim=1)  # Global average pooling over time\n",
    "        return self.classifier(pooled)  # Shape: [batch, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e07b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(ids, train, val, test):\n",
    "    # proportions of train, val, test\n",
    "    assert (train+val+test == 1)\n",
    "    IDs = np.unique(ids)\n",
    "    num_ids = len(IDs)\n",
    "\n",
    "    # priority given to the test/val sets\n",
    "    test_split = math.ceil(test * num_ids)\n",
    "    val_split = math.ceil(val * num_ids)\n",
    "    train_split = num_ids - val_split - test_split\n",
    "\n",
    "    train = np.where(np.isin(ids, IDs[:train_split]))[0]\n",
    "    val = np.where(np.isin(ids, IDs[train_split:train_split+val_split]))[0]\n",
    "    test = np.where(np.isin(ids, IDs[train_split+val_split:]))[0]\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1c030861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:(21042, 500, 129) y_train.shape: (21042,)\n",
      "X_val.shape:(4980, 500, 129) y_val.shape: (4980,)\n",
      "X_test.shape:(4820, 500, 129) y_test.shape: (4820,)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "train, val, test = split(ids, 0.7, 0.15, 0.15)\n",
    "X_train, y_train = trainX[train], trainY[train]\n",
    "X_val, y_val = trainX[val], trainY[val]\n",
    "X_test, y_test = trainX[test], trainY[test]\n",
    "\n",
    "print(f\"X_train.shape:{X_train.shape} y_train.shape: {y_train.shape}\")\n",
    "print(f\"X_val.shape:{X_val.shape} y_val.shape: {y_val.shape}\")\n",
    "print(f\"X_test.shape:{X_test.shape} y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "37405250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape: (N, 1)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f3d9dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "model = SimpleEncoder().to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ae9c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create logs directory if not exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Set filename based on current date and time\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_filename = f\"logs/train_log_{timestamp}.log\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Also print to console\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6597bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 04:19:06,636 - INFO - Epoch 1/50 | Train Loss: 0.0931 | Val Acc: 97.8514 | Test Acc: 96.6390\n",
      "2025-04-19 04:19:25,140 - INFO - Epoch 2/50 | Train Loss: 0.0844 | Val Acc: 97.2691 | Test Acc: 96.9295\n",
      "2025-04-19 04:19:42,723 - INFO - Epoch 3/50 | Train Loss: 0.0831 | Val Acc: 97.1084 | Test Acc: 95.8091\n",
      "2025-04-19 04:20:00,850 - INFO - Epoch 4/50 | Train Loss: 0.0734 | Val Acc: 97.0482 | Test Acc: 96.5768\n",
      "2025-04-19 04:20:18,970 - INFO - Epoch 5/50 | Train Loss: 0.0733 | Val Acc: 97.6305 | Test Acc: 96.8880\n",
      "2025-04-19 04:20:37,117 - INFO - Epoch 6/50 | Train Loss: 0.0682 | Val Acc: 97.4096 | Test Acc: 96.7635\n",
      "2025-04-19 04:20:55,465 - INFO - Epoch 7/50 | Train Loss: 0.0710 | Val Acc: 96.8876 | Test Acc: 95.7469\n",
      "2025-04-19 04:21:13,578 - INFO - Epoch 8/50 | Train Loss: 0.0645 | Val Acc: 97.7309 | Test Acc: 97.1162\n",
      "2025-04-19 04:21:32,143 - INFO - Epoch 9/50 | Train Loss: 0.0567 | Val Acc: 97.7510 | Test Acc: 96.8050\n",
      "2025-04-19 04:21:50,279 - INFO - Epoch 10/50 | Train Loss: 0.0538 | Val Acc: 97.2691 | Test Acc: 96.5353\n",
      "2025-04-19 04:22:08,404 - INFO - Epoch 11/50 | Train Loss: 0.0524 | Val Acc: 97.3494 | Test Acc: 97.1162\n",
      "2025-04-19 04:22:26,932 - INFO - Epoch 12/50 | Train Loss: 0.0540 | Val Acc: 97.0080 | Test Acc: 95.8921\n",
      "2025-04-19 04:22:45,280 - INFO - Epoch 13/50 | Train Loss: 0.0466 | Val Acc: 97.3695 | Test Acc: 96.7012\n",
      "2025-04-19 04:23:03,765 - INFO - Epoch 14/50 | Train Loss: 0.0474 | Val Acc: 97.1084 | Test Acc: 96.7012\n",
      "2025-04-19 04:23:22,531 - INFO - Epoch 15/50 | Train Loss: 0.0494 | Val Acc: 97.3896 | Test Acc: 96.9295\n",
      "2025-04-19 04:23:40,949 - INFO - Epoch 16/50 | Train Loss: 0.0462 | Val Acc: 97.6707 | Test Acc: 97.0332\n",
      "2025-04-19 04:23:59,838 - INFO - Epoch 17/50 | Train Loss: 0.0406 | Val Acc: 97.4096 | Test Acc: 96.4938\n",
      "2025-04-19 04:24:18,297 - INFO - Epoch 18/50 | Train Loss: 0.0433 | Val Acc: 97.4699 | Test Acc: 96.5560\n",
      "2025-04-19 04:24:36,703 - INFO - Epoch 19/50 | Train Loss: 0.0400 | Val Acc: 97.3896 | Test Acc: 96.5560\n",
      "2025-04-19 04:24:55,057 - INFO - Epoch 20/50 | Train Loss: 0.0351 | Val Acc: 97.6104 | Test Acc: 97.0124\n",
      "2025-04-19 04:25:13,369 - INFO - Epoch 21/50 | Train Loss: 0.0395 | Val Acc: 97.2289 | Test Acc: 96.6390\n",
      "2025-04-19 04:25:31,669 - INFO - Epoch 22/50 | Train Loss: 0.0365 | Val Acc: 96.9478 | Test Acc: 96.5353\n",
      "2025-04-19 04:25:49,933 - INFO - Epoch 23/50 | Train Loss: 0.0328 | Val Acc: 96.8876 | Test Acc: 96.4315\n",
      "2025-04-19 04:26:08,261 - INFO - Epoch 24/50 | Train Loss: 0.0391 | Val Acc: 97.2490 | Test Acc: 96.8672\n",
      "2025-04-19 04:26:26,642 - INFO - Epoch 25/50 | Train Loss: 0.0302 | Val Acc: 97.2289 | Test Acc: 96.8672\n",
      "2025-04-19 04:26:45,207 - INFO - Epoch 26/50 | Train Loss: 0.0324 | Val Acc: 97.4498 | Test Acc: 97.0747\n",
      "2025-04-19 04:27:03,963 - INFO - Epoch 27/50 | Train Loss: 0.0307 | Val Acc: 97.2490 | Test Acc: 96.3071\n",
      "2025-04-19 04:27:22,713 - INFO - Epoch 28/50 | Train Loss: 0.0321 | Val Acc: 97.1888 | Test Acc: 96.2448\n",
      "2025-04-19 04:27:41,484 - INFO - Epoch 29/50 | Train Loss: 0.0272 | Val Acc: 96.9679 | Test Acc: 96.6390\n",
      "2025-04-19 04:28:00,165 - INFO - Epoch 30/50 | Train Loss: 0.0275 | Val Acc: 96.6867 | Test Acc: 95.9336\n",
      "2025-04-19 04:28:18,172 - INFO - Epoch 31/50 | Train Loss: 0.0303 | Val Acc: 97.0884 | Test Acc: 96.4315\n",
      "2025-04-19 04:28:35,691 - INFO - Epoch 32/50 | Train Loss: 0.0323 | Val Acc: 96.8876 | Test Acc: 95.9751\n",
      "2025-04-19 04:28:53,146 - INFO - Epoch 33/50 | Train Loss: 0.0276 | Val Acc: 96.8876 | Test Acc: 96.8465\n",
      "2025-04-19 04:29:10,742 - INFO - Epoch 34/50 | Train Loss: 0.0273 | Val Acc: 97.1888 | Test Acc: 96.5145\n",
      "2025-04-19 04:29:28,240 - INFO - Epoch 35/50 | Train Loss: 0.0255 | Val Acc: 96.5663 | Test Acc: 96.4108\n",
      "2025-04-19 04:29:47,474 - INFO - Epoch 36/50 | Train Loss: 0.0266 | Val Acc: 96.8675 | Test Acc: 96.0996\n",
      "2025-04-19 04:30:06,650 - INFO - Epoch 37/50 | Train Loss: 0.0233 | Val Acc: 96.2249 | Test Acc: 95.7884\n",
      "2025-04-19 04:30:25,794 - INFO - Epoch 38/50 | Train Loss: 0.0263 | Val Acc: 96.4659 | Test Acc: 96.1203\n",
      "2025-04-19 04:30:44,982 - INFO - Epoch 39/50 | Train Loss: 0.0273 | Val Acc: 97.3695 | Test Acc: 96.7220\n",
      "2025-04-19 04:31:04,200 - INFO - Epoch 40/50 | Train Loss: 0.0202 | Val Acc: 97.4498 | Test Acc: 96.3278\n",
      "2025-04-19 04:31:23,292 - INFO - Epoch 41/50 | Train Loss: 0.0192 | Val Acc: 96.8474 | Test Acc: 96.3278\n",
      "2025-04-19 04:31:42,384 - INFO - Epoch 42/50 | Train Loss: 0.0235 | Val Acc: 97.1687 | Test Acc: 95.9544\n",
      "2025-04-19 04:32:01,490 - INFO - Epoch 43/50 | Train Loss: 0.0216 | Val Acc: 97.2088 | Test Acc: 97.0539\n",
      "2025-04-19 04:32:20,608 - INFO - Epoch 44/50 | Train Loss: 0.0232 | Val Acc: 96.6064 | Test Acc: 96.3071\n",
      "2025-04-19 04:32:39,699 - INFO - Epoch 45/50 | Train Loss: 0.0243 | Val Acc: 97.4900 | Test Acc: 96.8880\n",
      "2025-04-19 04:32:58,793 - INFO - Epoch 46/50 | Train Loss: 0.0224 | Val Acc: 96.9880 | Test Acc: 96.5975\n",
      "2025-04-19 04:33:17,894 - INFO - Epoch 47/50 | Train Loss: 0.0183 | Val Acc: 97.1285 | Test Acc: 96.3693\n",
      "2025-04-19 04:33:37,028 - INFO - Epoch 48/50 | Train Loss: 0.0174 | Val Acc: 97.0482 | Test Acc: 95.0622\n",
      "2025-04-19 04:33:56,132 - INFO - Epoch 49/50 | Train Loss: 0.0239 | Val Acc: 97.2892 | Test Acc: 96.5560\n",
      "2025-04-19 04:34:15,239 - INFO - Epoch 50/50 | Train Loss: 0.0170 | Val Acc: 97.1285 | Test Acc: 96.3485\n",
      "2025-04-19 04:34:15,814 - INFO - \n",
      "✅ Best model saved as 'best_model.pt' with val acc: 97.8514%\n",
      "2025-04-19 04:34:15,860 - INFO - ✅ Encoder weights saved separately to 'pretrained_encoder.pt'\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        val_acc = correct / total\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "    # Test evaluation each epoch\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        test_acc = correct / total\n",
    "\n",
    "    logger.info(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Acc: {val_acc*100:.4f} | Test Acc: {test_acc*100:.4f}\")\n",
    "\n",
    "# Save best model to file\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    torch.save(best_model_state, \"best_model.pt\")\n",
    "    logger.info(f\"\\n✅ Best model saved as 'best_model.pt' with val acc: {best_val_acc*100:.4f}%\")\n",
    "\n",
    "    # Save just the encoder for reuse\n",
    "    torch.save(model.encoder.state_dict(), \"pretrained_encoder.pt\")\n",
    "    logger.info(\"✅ Encoder weights saved separately to 'pretrained_encoder.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and set to eval mode\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Move test data to device\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# Get predictions on full test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    preds = (outputs > 0.5).float()\n",
    "\n",
    "# Compute correct and wrong counts\n",
    "correct_preds = (preds == y_test_tensor).sum().item()\n",
    "total_preds = y_test_tensor.size(0)\n",
    "wrong_preds = total_preds - correct_preds\n",
    "\n",
    "# Print counts\n",
    "logger.info(f\"\\n✅ Total Correct Predictions: {correct_preds}\")\n",
    "logger.info(f\"❌ Total Wrong Predictions:   {wrong_preds}\")\n",
    "logger.info(f\"📊 Test Accuracy:             {(correct_preds / total_preds) * 100:.2f}%\")\n",
    "\n",
    "# Print predictions for first 10 test samples\n",
    "logger.info(\"\\n📊 Predictions vs Ground Truth for first 10 test samples:\\n\")\n",
    "for i in range(10):\n",
    "    pred_val = preds[i].item()\n",
    "    actual_val = y_test_tensor[i].item()\n",
    "    logger.info(f\"Sample {i+1:02d} | Predicted: {int(pred_val)} | Actual: {int(actual_val)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a7a56",
   "metadata": {},
   "source": [
    "# TASK 2: Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbaf5d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Aniket Konkar\\AppData\\Local\\Temp\\ipykernel_6988\\4130880486.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  data = np.load('data\\Direction_task_with_dots_synchronised_min_15_perc.npz')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX.shape: (2674, 500, 129)\n",
      "trainY.shape: (2674, 2)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data\\Direction_task_with_dots_synchronised_min_15_perc.npz')\n",
    "\n",
    "trainX = data['EEG']\n",
    "trainY = data['labels'][:, 1:3]\n",
    "ids = data['labels'][:, 0] # ID\n",
    "print(f\"trainX.shape: {trainX.shape}\")\n",
    "print(f\"trainY.shape: {trainY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef51140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:(2157, 500, 129) y_train.shape: (2157, 2)\n",
      "X_val.shape:(233, 500, 129) y_val.shape: (233, 2)\n",
      "X_test.shape:(284, 500, 129) y_test.shape: (284, 2)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "train, val, test = split(ids, 0.8, 0.1, 0.1)\n",
    "X_train, y_train = trainX[train], trainY[train]\n",
    "X_val, y_val = trainX[val], trainY[val]\n",
    "X_test, y_test = trainX[test], trainY[test]\n",
    "\n",
    "print(f\"X_train.shape:{X_train.shape} y_train.shape: {y_train.shape}\")\n",
    "print(f\"X_val.shape:{X_val.shape} y_val.shape: {y_val.shape}\")\n",
    "print(f\"X_test.shape:{X_test.shape} y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29172be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape: (N, 2)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5466a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRegressor(nn.Module):\n",
    "    def __init__(self, encoder, input_dim=129):\n",
    "        super(MultiTaskRegressor, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Shared head for feature extraction\n",
    "        self.shared_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Separate heads for amplitude and angle\n",
    "        self.amplitude_head = nn.Linear(128, 1)\n",
    "        self.angle_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder.encoder(x)  # Use encoder.encoder to get the transformer output\n",
    "        pooled = encoded.mean(dim=1)       # Same as in SimpleEncoder\n",
    "        features = self.shared_head(pooled)\n",
    "        amplitude = self.amplitude_head(features)\n",
    "        angle = self.angle_head(features)\n",
    "        return amplitude, angle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecccd8",
   "metadata": {},
   "source": [
    "# MultiTaskRegressor Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aniket Konkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Load encoder and its weights\n",
    "encoder = SimpleEncoder(input_dim=129).to(device)\n",
    "state_dict = torch.load(\"pretrained_encoder.pt\", map_location=device)\n",
    "encoder.encoder.load_state_dict(state_dict)\n",
    "\n",
    "# Wrap in multitask regressor\n",
    "model = MultiTaskRegressor(encoder=encoder, input_dim=129).to(device)\n",
    "\n",
    "# Angle loss with correct angle error formula (torch)\n",
    "criterion_angle = lambda pred, target: torch.mean(\n",
    "    torch.square(torch.atan2(torch.sin(target - pred), torch.cos(target - pred)))\n",
    ")\n",
    "\n",
    "# Define loss functions\n",
    "criterion_amplitude = nn.MSELoss()\n",
    "\n",
    "# Choose weighting method\n",
    "learn_uncertainty = True  # ← set to False if you want fixed weights\n",
    "\n",
    "if learn_uncertainty:\n",
    "    # Learnable log variances for adaptive weighting\n",
    "    log_sigma_amp = torch.nn.Parameter(torch.tensor(0.0, requires_grad=True, device=device))\n",
    "    log_sigma_ang = torch.nn.Parameter(torch.tensor(0.0, requires_grad=True, device=device))\n",
    "    optimizer = optim.Adam(\n",
    "        list(model.parameters()) + [log_sigma_amp, log_sigma_ang], lr=1e-4\n",
    "    )\n",
    "else:\n",
    "    # Manual weights\n",
    "    w_amp = 1.0\n",
    "    w_ang = 10000.0  # Tune based on your data\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # Define Optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10549c5a",
   "metadata": {},
   "source": [
    "# Training for MultiTaskRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "591e9100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aniket Konkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Aniket Konkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([17])) that is different to the input size (torch.Size([17, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Aniket Konkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([11])) that is different to the input size (torch.Size([11, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Aniket Konkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "2025-04-19 05:16:45,888 - INFO - Epoch 1/50\n",
      "2025-04-19 05:16:46,251 - INFO - 🔹 Train Loss: 59230.2428 (Amplitude: 119765.0651, Angle: 3.2758)\n",
      "2025-04-19 05:16:46,252 - INFO - 🔸 Val   Loss: 53101.6296 (Amplitude: 108577.6045, Angle: 2.8294)\n",
      "2025-04-19 05:16:46,253 - INFO - 🔻 Test  Loss: 46237.5209 (Amplitude: 94541.8933, Angle: 3.0339)\n",
      "2025-04-19 05:16:46,277 - INFO -    ↪ log_sigma_amp: 0.0221, log_sigma_ang: 0.0201\n",
      "2025-04-19 05:16:46,278 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:17:08,583 - INFO - Epoch 2/50\n",
      "2025-04-19 05:17:08,591 - INFO - 🔹 Train Loss: 51394.1584 (Amplitude: 106136.5641, Angle: 2.9565)\n",
      "2025-04-19 05:17:08,591 - INFO - 🔸 Val   Loss: 43457.0700 (Amplitude: 90673.4280, Angle: 2.8278)\n",
      "2025-04-19 05:17:08,634 - INFO - 🔻 Test  Loss: 37250.9314 (Amplitude: 77723.6603, Angle: 3.0317)\n",
      "2025-04-19 05:17:08,634 - INFO -    ↪ log_sigma_amp: 0.0424, log_sigma_ang: 0.0397\n",
      "2025-04-19 05:17:08,634 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:17:29,997 - INFO - Epoch 3/50\n",
      "2025-04-19 05:17:29,998 - INFO - 🔹 Train Loss: 40183.1441 (Amplitude: 84556.1774, Angle: 2.9573)\n",
      "2025-04-19 05:17:30,019 - INFO - 🔸 Val   Loss: 31559.3595 (Amplitude: 67001.6361, Angle: 2.8335)\n",
      "2025-04-19 05:17:30,019 - INFO - 🔻 Test  Loss: 26259.4181 (Amplitude: 55748.9798, Angle: 3.0284)\n",
      "2025-04-19 05:17:30,060 - INFO -    ↪ log_sigma_amp: 0.0597, log_sigma_ang: 0.0599\n",
      "2025-04-19 05:17:30,060 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:17:50,966 - INFO - Epoch 4/50\n",
      "2025-04-19 05:17:50,968 - INFO - 🔹 Train Loss: 28179.6447 (Amplitude: 60226.6330, Angle: 2.9577)\n",
      "2025-04-19 05:17:50,968 - INFO - 🔸 Val   Loss: 20849.2369 (Amplitude: 44874.1740, Angle: 2.8411)\n",
      "2025-04-19 05:17:50,969 - INFO - 🔻 Test  Loss: 16600.8269 (Amplitude: 35729.4421, Angle: 3.0386)\n",
      "2025-04-19 05:17:50,970 - INFO -    ↪ log_sigma_amp: 0.0735, log_sigma_ang: 0.0804\n",
      "2025-04-19 05:17:51,002 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:18:12,076 - INFO - Epoch 5/50\n",
      "2025-04-19 05:18:12,077 - INFO - 🔹 Train Loss: 18720.8926 (Amplitude: 40498.3731, Angle: 2.9633)\n",
      "2025-04-19 05:18:12,078 - INFO - 🔸 Val   Loss: 14129.2177 (Amplitude: 30728.7917, Angle: 2.8302)\n",
      "2025-04-19 05:18:12,078 - INFO - 🔻 Test  Loss: 10897.2780 (Amplitude: 23698.9559, Angle: 3.0340)\n",
      "2025-04-19 05:18:12,078 - INFO -    ↪ log_sigma_amp: 0.0839, log_sigma_ang: 0.1008\n",
      "2025-04-19 05:18:12,079 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:18:32,667 - INFO - Epoch 6/50\n",
      "2025-04-19 05:18:32,668 - INFO - 🔹 Train Loss: 13584.0203 (Amplitude: 29668.1243, Angle: 2.9680)\n",
      "2025-04-19 05:18:32,668 - INFO - 🔸 Val   Loss: 11553.1100 (Amplitude: 25344.3989, Angle: 2.8326)\n",
      "2025-04-19 05:18:32,668 - INFO - 🔻 Test  Loss: 9084.8101 (Amplitude: 19928.7873, Angle: 3.0284)\n",
      "2025-04-19 05:18:32,669 - INFO -    ↪ log_sigma_amp: 0.0926, log_sigma_ang: 0.1211\n",
      "2025-04-19 05:18:32,669 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:18:53,577 - INFO - Epoch 7/50\n",
      "2025-04-19 05:18:53,588 - INFO - 🔹 Train Loss: 11837.5975 (Amplitude: 26076.7040, Angle: 2.9544)\n",
      "2025-04-19 05:18:53,589 - INFO - 🔸 Val   Loss: 11021.7682 (Amplitude: 24385.9240, Angle: 2.8500)\n",
      "2025-04-19 05:18:53,589 - INFO - 🔻 Test  Loss: 8973.0499 (Amplitude: 19852.3498, Angle: 3.0346)\n",
      "2025-04-19 05:18:53,590 - INFO -    ↪ log_sigma_amp: 0.1011, log_sigma_ang: 0.1411\n",
      "2025-04-19 05:18:53,590 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:19:14,048 - INFO - Epoch 8/50\n",
      "2025-04-19 05:19:14,049 - INFO - 🔹 Train Loss: 11415.0014 (Amplitude: 25370.6198, Angle: 2.9615)\n",
      "2025-04-19 05:19:14,071 - INFO - 🔸 Val   Loss: 10929.9930 (Amplitude: 24408.8483, Angle: 2.8586)\n",
      "2025-04-19 05:19:14,072 - INFO - 🔻 Test  Loss: 9073.2656 (Amplitude: 20261.7215, Angle: 3.0399)\n",
      "2025-04-19 05:19:14,091 - INFO -    ↪ log_sigma_amp: 0.1104, log_sigma_ang: 0.1612\n",
      "2025-04-19 05:19:14,091 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:19:34,537 - INFO - Epoch 9/50\n",
      "2025-04-19 05:19:34,537 - INFO - 🔹 Train Loss: 11268.5073 (Amplitude: 25290.2399, Angle: 2.9609)\n",
      "2025-04-19 05:19:34,538 - INFO - 🔸 Val   Loss: 10849.6544 (Amplitude: 24478.2371, Angle: 2.8335)\n",
      "2025-04-19 05:19:34,538 - INFO - 🔻 Test  Loss: 9070.2539 (Amplitude: 20462.9942, Angle: 3.0349)\n",
      "2025-04-19 05:19:34,539 - INFO -    ↪ log_sigma_amp: 0.1206, log_sigma_ang: 0.1810\n",
      "2025-04-19 05:19:34,539 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:19:54,519 - INFO - Epoch 10/50\n",
      "2025-04-19 05:19:54,520 - INFO - 🔹 Train Loss: 11141.9292 (Amplitude: 25274.6402, Angle: 2.9608)\n",
      "2025-04-19 05:19:54,520 - INFO - 🔸 Val   Loss: 10734.8506 (Amplitude: 24490.8694, Angle: 2.8277)\n",
      "2025-04-19 05:19:54,521 - INFO - 🔻 Test  Loss: 8983.5216 (Amplitude: 20494.6418, Angle: 3.0321)\n",
      "2025-04-19 05:19:54,534 - INFO -    ↪ log_sigma_amp: 0.1318, log_sigma_ang: 0.2008\n",
      "2025-04-19 05:19:54,534 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:20:14,517 - INFO - Epoch 11/50\n",
      "2025-04-19 05:20:14,517 - INFO - 🔹 Train Loss: 11013.0334 (Amplitude: 25273.9756, Angle: 2.9625)\n",
      "2025-04-19 05:20:14,518 - INFO - 🔸 Val   Loss: 10615.0839 (Amplitude: 24511.3953, Angle: 2.8280)\n",
      "2025-04-19 05:20:14,518 - INFO - 🔻 Test  Loss: 8897.3494 (Amplitude: 20544.2762, Angle: 3.0325)\n",
      "2025-04-19 05:20:14,530 - INFO -    ↪ log_sigma_amp: 0.1438, log_sigma_ang: 0.2204\n",
      "2025-04-19 05:20:14,531 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:20:34,974 - INFO - Epoch 12/50\n",
      "2025-04-19 05:20:34,975 - INFO - 🔹 Train Loss: 10876.3893 (Amplitude: 25275.2730, Angle: 2.9633)\n",
      "2025-04-19 05:20:34,975 - INFO - 🔸 Val   Loss: 10479.2849 (Amplitude: 24514.2718, Angle: 2.8774)\n",
      "2025-04-19 05:20:34,976 - INFO - 🔻 Test  Loss: 8785.4131 (Amplitude: 20551.1087, Angle: 3.0651)\n",
      "2025-04-19 05:20:34,976 - INFO -    ↪ log_sigma_amp: 0.1568, log_sigma_ang: 0.2399\n",
      "2025-04-19 05:20:34,977 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:20:55,417 - INFO - Epoch 13/50\n",
      "2025-04-19 05:20:55,418 - INFO - 🔹 Train Loss: 10731.8527 (Amplitude: 25275.6593, Angle: 2.9649)\n",
      "2025-04-19 05:20:55,418 - INFO - 🔸 Val   Loss: 10331.7406 (Amplitude: 24505.0982, Angle: 2.8282)\n",
      "2025-04-19 05:20:55,419 - INFO - 🔻 Test  Loss: 8655.7975 (Amplitude: 20529.3619, Angle: 3.0328)\n",
      "2025-04-19 05:20:55,419 - INFO -    ↪ log_sigma_amp: 0.1706, log_sigma_ang: 0.2593\n",
      "2025-04-19 05:20:55,420 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:21:15,371 - INFO - Epoch 14/50\n",
      "2025-04-19 05:21:15,371 - INFO - 🔹 Train Loss: 10580.4154 (Amplitude: 25275.9254, Angle: 2.9623)\n",
      "2025-04-19 05:21:15,372 - INFO - 🔸 Val   Loss: 10180.7439 (Amplitude: 24502.7281, Angle: 2.8371)\n",
      "2025-04-19 05:21:15,372 - INFO - 🔻 Test  Loss: 8527.8103 (Amplitude: 20523.8095, Angle: 3.0286)\n",
      "2025-04-19 05:21:15,373 - INFO -    ↪ log_sigma_amp: 0.1853, log_sigma_ang: 0.2786\n",
      "2025-04-19 05:21:15,373 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:21:35,034 - INFO - Epoch 15/50\n",
      "2025-04-19 05:21:35,034 - INFO - 🔹 Train Loss: 10421.9638 (Amplitude: 25273.3971, Angle: 2.9610)\n",
      "2025-04-19 05:21:35,059 - INFO - 🔸 Val   Loss: 10014.4702 (Amplitude: 24475.7802, Angle: 2.8491)\n",
      "2025-04-19 05:21:35,059 - INFO - 🔻 Test  Loss: 8370.3910 (Amplitude: 20456.8816, Angle: 3.0437)\n",
      "2025-04-19 05:21:35,060 - INFO -    ↪ log_sigma_amp: 0.2006, log_sigma_ang: 0.2977\n",
      "2025-04-19 05:21:35,061 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:21:54,390 - INFO - Epoch 16/50\n",
      "2025-04-19 05:21:54,390 - INFO - 🔹 Train Loss: 10259.9641 (Amplitude: 25275.4768, Angle: 2.9634)\n",
      "2025-04-19 05:21:54,391 - INFO - 🔸 Val   Loss: 9862.9923 (Amplitude: 24495.1183, Angle: 2.8277)\n",
      "2025-04-19 05:21:54,391 - INFO - 🔻 Test  Loss: 8256.7452 (Amplitude: 20505.2333, Angle: 3.0318)\n",
      "2025-04-19 05:21:54,392 - INFO -    ↪ log_sigma_amp: 0.2167, log_sigma_ang: 0.3168\n",
      "2025-04-19 05:21:54,392 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:22:14,062 - INFO - Epoch 17/50\n",
      "2025-04-19 05:22:14,062 - INFO - 🔹 Train Loss: 10092.7980 (Amplitude: 25273.4918, Angle: 2.9625)\n",
      "2025-04-19 05:22:14,095 - INFO - 🔸 Val   Loss: 9701.9343 (Amplitude: 24501.9667, Angle: 2.8466)\n",
      "2025-04-19 05:22:14,096 - INFO - 🔻 Test  Loss: 8126.2479 (Amplitude: 20521.9034, Angle: 3.0421)\n",
      "2025-04-19 05:22:14,096 - INFO -    ↪ log_sigma_amp: 0.2334, log_sigma_ang: 0.3357\n",
      "2025-04-19 05:22:14,096 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:22:33,471 - INFO - Epoch 18/50\n",
      "2025-04-19 05:22:33,472 - INFO - 🔹 Train Loss: 9924.9990 (Amplitude: 25277.5666, Angle: 2.9625)\n",
      "2025-04-19 05:22:33,472 - INFO - 🔸 Val   Loss: 9539.0320 (Amplitude: 24507.6407, Angle: 2.8379)\n",
      "2025-04-19 05:22:33,473 - INFO - 🔻 Test  Loss: 7993.2848 (Amplitude: 20535.6007, Angle: 3.0288)\n",
      "2025-04-19 05:22:33,474 - INFO -    ↪ log_sigma_amp: 0.2506, log_sigma_ang: 0.3546\n",
      "2025-04-19 05:22:33,475 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:22:52,958 - INFO - Epoch 19/50\n",
      "2025-04-19 05:22:52,958 - INFO - 🔹 Train Loss: 9752.7094 (Amplitude: 25274.9986, Angle: 2.9571)\n",
      "2025-04-19 05:22:52,960 - INFO - 🔸 Val   Loss: 9370.9043 (Amplitude: 24503.0524, Angle: 2.8309)\n",
      "2025-04-19 05:22:52,960 - INFO - 🔻 Test  Loss: 7849.6604 (Amplitude: 20524.5685, Angle: 3.0342)\n",
      "2025-04-19 05:22:52,961 - INFO -    ↪ log_sigma_amp: 0.2682, log_sigma_ang: 0.3734\n",
      "2025-04-19 05:22:52,961 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:23:13,394 - INFO - Epoch 20/50\n",
      "2025-04-19 05:23:13,394 - INFO - 🔹 Train Loss: 9580.4320 (Amplitude: 25276.4481, Angle: 2.9589)\n",
      "2025-04-19 05:23:13,396 - INFO - 🔸 Val   Loss: 9205.0159 (Amplitude: 24508.5544, Angle: 2.8681)\n",
      "2025-04-19 05:23:13,396 - INFO - 🔻 Test  Loss: 7713.9250 (Amplitude: 20537.7680, Angle: 3.0461)\n",
      "2025-04-19 05:23:13,397 - INFO -    ↪ log_sigma_amp: 0.2863, log_sigma_ang: 0.3920\n",
      "2025-04-19 05:23:13,397 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:23:33,091 - INFO - Epoch 21/50\n",
      "2025-04-19 05:23:33,092 - INFO - 🔹 Train Loss: 9407.7194 (Amplitude: 25278.0138, Angle: 2.9603)\n",
      "2025-04-19 05:23:33,093 - INFO - 🔸 Val   Loss: 9038.2667 (Amplitude: 24512.8078, Angle: 2.8283)\n",
      "2025-04-19 05:23:33,093 - INFO - 🔻 Test  Loss: 7576.5895 (Amplitude: 20547.8223, Angle: 3.0308)\n",
      "2025-04-19 05:23:33,094 - INFO -    ↪ log_sigma_amp: 0.3047, log_sigma_ang: 0.4106\n",
      "2025-04-19 05:23:33,094 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:23:52,871 - INFO - Epoch 22/50\n",
      "2025-04-19 05:23:52,871 - INFO - 🔹 Train Loss: 9234.6157 (Amplitude: 25278.0220, Angle: 2.9575)\n",
      "2025-04-19 05:23:52,872 - INFO - 🔸 Val   Loss: 8877.0591 (Amplitude: 24530.3539, Angle: 2.8368)\n",
      "2025-04-19 05:23:52,872 - INFO - 🔻 Test  Loss: 7450.9412 (Amplitude: 20588.7516, Angle: 3.0286)\n",
      "2025-04-19 05:23:52,874 - INFO -    ↪ log_sigma_amp: 0.3234, log_sigma_ang: 0.4291\n",
      "2025-04-19 05:23:52,874 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:24:11,554 - INFO - Epoch 23/50\n",
      "2025-04-19 05:24:11,554 - INFO - 🔹 Train Loss: 9061.4897 (Amplitude: 25276.1067, Angle: 2.9583)\n",
      "2025-04-19 05:24:11,555 - INFO - 🔸 Val   Loss: 8706.5615 (Amplitude: 24520.4542, Angle: 2.8349)\n",
      "2025-04-19 05:24:11,555 - INFO - 🔻 Test  Loss: 7302.6568 (Amplitude: 20565.8382, Angle: 3.0356)\n",
      "2025-04-19 05:24:11,556 - INFO -    ↪ log_sigma_amp: 0.3424, log_sigma_ang: 0.4474\n",
      "2025-04-19 05:24:11,567 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:24:30,379 - INFO - Epoch 24/50\n",
      "2025-04-19 05:24:30,380 - INFO - 🔹 Train Loss: 8889.8078 (Amplitude: 25275.0796, Angle: 2.9656)\n",
      "2025-04-19 05:24:30,381 - INFO - 🔸 Val   Loss: 8540.3828 (Amplitude: 24518.5822, Angle: 2.8374)\n",
      "2025-04-19 05:24:30,381 - INFO - 🔻 Test  Loss: 7162.2933 (Amplitude: 20561.4495, Angle: 3.0287)\n",
      "2025-04-19 05:24:30,382 - INFO -    ↪ log_sigma_amp: 0.3616, log_sigma_ang: 0.4659\n",
      "2025-04-19 05:24:30,382 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:24:49,972 - INFO - Epoch 25/50\n",
      "2025-04-19 05:24:49,973 - INFO - 🔹 Train Loss: 8720.7999 (Amplitude: 25276.4470, Angle: 2.9597)\n",
      "2025-04-19 05:24:49,974 - INFO - 🔸 Val   Loss: 8373.5897 (Amplitude: 24508.7778, Angle: 2.8449)\n",
      "2025-04-19 05:24:49,974 - INFO - 🔻 Test  Loss: 7017.3469 (Amplitude: 20538.3827, Angle: 3.0316)\n",
      "2025-04-19 05:24:49,975 - INFO -    ↪ log_sigma_amp: 0.3810, log_sigma_ang: 0.4841\n",
      "2025-04-19 05:24:49,975 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:25:08,748 - INFO - Epoch 26/50\n",
      "2025-04-19 05:25:08,749 - INFO - 🔹 Train Loss: 8551.9796 (Amplitude: 25273.0777, Angle: 2.9598)\n",
      "2025-04-19 05:25:08,749 - INFO - 🔸 Val   Loss: 8200.4926 (Amplitude: 24474.0005, Angle: 2.8283)\n",
      "2025-04-19 05:25:08,750 - INFO - 🔻 Test  Loss: 6853.2312 (Amplitude: 20452.3340, Angle: 3.0329)\n",
      "2025-04-19 05:25:08,750 - INFO -    ↪ log_sigma_amp: 0.4004, log_sigma_ang: 0.5023\n",
      "2025-04-19 05:25:08,751 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:25:27,738 - INFO - Epoch 27/50\n",
      "2025-04-19 05:25:27,739 - INFO - 🔹 Train Loss: 8387.4819 (Amplitude: 25276.5656, Angle: 2.9582)\n",
      "2025-04-19 05:25:27,739 - INFO - 🔸 Val   Loss: 8052.8359 (Amplitude: 24510.1783, Angle: 2.8357)\n",
      "2025-04-19 05:25:27,740 - INFO - 🔻 Test  Loss: 6749.2739 (Amplitude: 20541.7449, Angle: 3.0284)\n",
      "2025-04-19 05:25:27,740 - INFO -    ↪ log_sigma_amp: 0.4201, log_sigma_ang: 0.5204\n",
      "2025-04-19 05:25:27,741 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:25:46,770 - INFO - Epoch 28/50\n",
      "2025-04-19 05:25:46,770 - INFO - 🔹 Train Loss: 8223.9125 (Amplitude: 25275.6724, Angle: 2.9662)\n",
      "2025-04-19 05:25:46,771 - INFO - 🔸 Val   Loss: 7904.6011 (Amplitude: 24537.2363, Angle: 2.8337)\n",
      "2025-04-19 05:25:46,771 - INFO - 🔻 Test  Loss: 6637.8953 (Amplitude: 20604.3333, Angle: 3.0284)\n",
      "2025-04-19 05:25:46,771 - INFO -    ↪ log_sigma_amp: 0.4398, log_sigma_ang: 0.5383\n",
      "2025-04-19 05:25:46,772 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:26:05,374 - INFO - Epoch 29/50\n",
      "2025-04-19 05:26:05,374 - INFO - 🔹 Train Loss: 8062.4852 (Amplitude: 25274.6155, Angle: 2.9607)\n",
      "2025-04-19 05:26:05,375 - INFO - 🔸 Val   Loss: 7735.1269 (Amplitude: 24491.5007, Angle: 2.8288)\n",
      "2025-04-19 05:26:05,375 - INFO - 🔻 Test  Loss: 6473.8185 (Amplitude: 20496.9935, Angle: 3.0302)\n",
      "2025-04-19 05:26:05,376 - INFO -    ↪ log_sigma_amp: 0.4596, log_sigma_ang: 0.5563\n",
      "2025-04-19 05:26:05,376 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:26:24,181 - INFO - Epoch 30/50\n",
      "2025-04-19 05:26:24,182 - INFO - 🔹 Train Loss: 7904.8260 (Amplitude: 25276.3749, Angle: 2.9566)\n",
      "2025-04-19 05:26:24,183 - INFO - 🔸 Val   Loss: 7590.9826 (Amplitude: 24516.8937, Angle: 2.8431)\n",
      "2025-04-19 05:26:24,183 - INFO - 🔻 Test  Loss: 6365.2937 (Amplitude: 20557.3752, Angle: 3.0307)\n",
      "2025-04-19 05:26:24,184 - INFO -    ↪ log_sigma_amp: 0.4794, log_sigma_ang: 0.5739\n",
      "2025-04-19 05:26:24,184 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:26:43,169 - INFO - Epoch 31/50\n",
      "2025-04-19 05:26:43,169 - INFO - 🔹 Train Loss: 7748.7673 (Amplitude: 25275.4569, Angle: 2.9563)\n",
      "2025-04-19 05:26:43,170 - INFO - 🔸 Val   Loss: 7439.4238 (Amplitude: 24511.2648, Angle: 2.8281)\n",
      "2025-04-19 05:26:43,170 - INFO - 🔻 Test  Loss: 6235.6006 (Amplitude: 20544.0377, Angle: 3.0327)\n",
      "2025-04-19 05:26:43,171 - INFO -    ↪ log_sigma_amp: 0.4994, log_sigma_ang: 0.5916\n",
      "2025-04-19 05:26:43,171 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:27:02,247 - INFO - Epoch 32/50\n",
      "2025-04-19 05:27:02,248 - INFO - 🔹 Train Loss: 7595.7202 (Amplitude: 25275.7026, Angle: 2.9625)\n",
      "2025-04-19 05:27:02,248 - INFO - 🔸 Val   Loss: 7293.7953 (Amplitude: 24516.1713, Angle: 2.8824)\n",
      "2025-04-19 05:27:02,249 - INFO - 🔻 Test  Loss: 6115.7733 (Amplitude: 20555.6578, Angle: 3.0693)\n",
      "2025-04-19 05:27:02,249 - INFO -    ↪ log_sigma_amp: 0.5193, log_sigma_ang: 0.6092\n",
      "2025-04-19 05:27:02,250 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:27:20,756 - INFO - Epoch 33/50\n",
      "2025-04-19 05:27:20,757 - INFO - 🔹 Train Loss: 7445.3196 (Amplitude: 25275.5086, Angle: 2.9690)\n",
      "2025-04-19 05:27:20,757 - INFO - 🔸 Val   Loss: 7161.6608 (Amplitude: 24559.4766, Angle: 2.8360)\n",
      "2025-04-19 05:27:20,758 - INFO - 🔻 Test  Loss: 6022.7716 (Amplitude: 20652.9775, Angle: 3.0285)\n",
      "2025-04-19 05:27:20,758 - INFO -    ↪ log_sigma_amp: 0.5394, log_sigma_ang: 0.6267\n",
      "2025-04-19 05:27:20,759 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:27:39,152 - INFO - Epoch 34/50\n",
      "2025-04-19 05:27:39,153 - INFO - 🔹 Train Loss: 7297.8211 (Amplitude: 25275.3399, Angle: 2.9642)\n",
      "2025-04-19 05:27:39,153 - INFO - 🔸 Val   Loss: 7000.9391 (Amplitude: 24493.5054, Angle: 2.8301)\n",
      "2025-04-19 05:27:39,154 - INFO - 🔻 Test  Loss: 5860.1816 (Amplitude: 20501.4944, Angle: 3.0341)\n",
      "2025-04-19 05:27:39,154 - INFO -    ↪ log_sigma_amp: 0.5594, log_sigma_ang: 0.6442\n",
      "2025-04-19 05:27:39,155 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:27:57,783 - INFO - Epoch 35/50\n",
      "2025-04-19 05:27:57,784 - INFO - 🔹 Train Loss: 7153.9011 (Amplitude: 25278.2570, Angle: 2.9628)\n",
      "2025-04-19 05:27:57,784 - INFO - 🔸 Val   Loss: 6856.7790 (Amplitude: 24474.7545, Angle: 2.8383)\n",
      "2025-04-19 05:27:57,785 - INFO - 🔻 Test  Loss: 5732.4427 (Amplitude: 20460.5537, Angle: 3.0290)\n",
      "2025-04-19 05:27:57,785 - INFO -    ↪ log_sigma_amp: 0.5795, log_sigma_ang: 0.6614\n",
      "2025-04-19 05:27:57,786 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:28:16,213 - INFO - Epoch 36/50\n",
      "2025-04-19 05:28:16,214 - INFO - 🔹 Train Loss: 7011.8021 (Amplitude: 25277.6354, Angle: 2.9582)\n",
      "2025-04-19 05:28:16,214 - INFO - 🔸 Val   Loss: 6721.9580 (Amplitude: 24478.7939, Angle: 2.8415)\n",
      "2025-04-19 05:28:16,215 - INFO - 🔻 Test  Loss: 5619.8912 (Amplitude: 20464.5028, Angle: 3.0301)\n",
      "2025-04-19 05:28:16,215 - INFO -    ↪ log_sigma_amp: 0.5995, log_sigma_ang: 0.6784\n",
      "2025-04-19 05:28:16,230 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:28:34,654 - INFO - Epoch 37/50\n",
      "2025-04-19 05:28:34,655 - INFO - 🔹 Train Loss: 6871.3007 (Amplitude: 25274.0616, Angle: 2.9600)\n",
      "2025-04-19 05:28:34,656 - INFO - 🔸 Val   Loss: 6617.3778 (Amplitude: 24588.4051, Angle: 2.8282)\n",
      "2025-04-19 05:28:34,656 - INFO - 🔻 Test  Loss: 5575.5383 (Amplitude: 20716.2210, Angle: 3.0328)\n",
      "2025-04-19 05:28:34,657 - INFO -    ↪ log_sigma_amp: 0.6196, log_sigma_ang: 0.6953\n",
      "2025-04-19 05:28:34,657 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:28:53,555 - INFO - Epoch 38/50\n",
      "2025-04-19 05:28:53,555 - INFO - 🔹 Train Loss: 6733.8127 (Amplitude: 25271.9048, Angle: 2.9610)\n",
      "2025-04-19 05:28:53,556 - INFO - 🔸 Val   Loss: 6458.1695 (Amplitude: 24484.2346, Angle: 2.8280)\n",
      "2025-04-19 05:28:53,556 - INFO - 🔻 Test  Loss: 5402.3642 (Amplitude: 20480.4238, Angle: 3.0316)\n",
      "2025-04-19 05:28:53,557 - INFO -    ↪ log_sigma_amp: 0.6397, log_sigma_ang: 0.7122\n",
      "2025-04-19 05:28:53,557 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:29:12,251 - INFO - Epoch 39/50\n",
      "2025-04-19 05:29:12,252 - INFO - 🔹 Train Loss: 6601.8550 (Amplitude: 25279.7361, Angle: 2.9588)\n",
      "2025-04-19 05:29:12,252 - INFO - 🔸 Val   Loss: 6330.7994 (Amplitude: 24488.5166, Angle: 2.8282)\n",
      "2025-04-19 05:29:12,253 - INFO - 🔻 Test  Loss: 5297.3912 (Amplitude: 20490.0729, Angle: 3.0327)\n",
      "2025-04-19 05:29:12,253 - INFO -    ↪ log_sigma_amp: 0.6598, log_sigma_ang: 0.7288\n",
      "2025-04-19 05:29:12,254 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:29:31,016 - INFO - Epoch 40/50\n",
      "2025-04-19 05:29:31,016 - INFO - 🔹 Train Loss: 6468.6468 (Amplitude: 25273.3302, Angle: 2.9542)\n",
      "2025-04-19 05:29:31,017 - INFO - 🔸 Val   Loss: 6197.0235 (Amplitude: 24458.8610, Angle: 2.8321)\n",
      "2025-04-19 05:29:31,017 - INFO - 🔻 Test  Loss: 5172.5927 (Amplitude: 20414.4753, Angle: 3.0287)\n",
      "2025-04-19 05:29:31,018 - INFO -    ↪ log_sigma_amp: 0.6800, log_sigma_ang: 0.7452\n",
      "2025-04-19 05:29:31,018 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:29:49,922 - INFO - Epoch 41/50\n",
      "2025-04-19 05:29:49,923 - INFO - 🔹 Train Loss: 6339.4101 (Amplitude: 25272.4219, Angle: 2.9554)\n",
      "2025-04-19 05:29:49,924 - INFO - 🔸 Val   Loss: 6099.1229 (Amplitude: 24562.3922, Angle: 2.8280)\n",
      "2025-04-19 05:29:49,924 - INFO - 🔻 Test  Loss: 5130.3704 (Amplitude: 20659.9501, Angle: 3.0313)\n",
      "2025-04-19 05:29:49,925 - INFO -    ↪ log_sigma_amp: 0.7002, log_sigma_ang: 0.7614\n",
      "2025-04-19 05:29:49,925 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:30:08,775 - INFO - Epoch 42/50\n",
      "2025-04-19 05:30:08,775 - INFO - 🔹 Train Loss: 6213.8265 (Amplitude: 25275.2681, Angle: 2.9775)\n",
      "2025-04-19 05:30:08,776 - INFO - 🔸 Val   Loss: 5975.4831 (Amplitude: 24554.8825, Angle: 2.8698)\n",
      "2025-04-19 05:30:08,776 - INFO - 🔻 Test  Loss: 5023.9861 (Amplitude: 20643.8333, Angle: 3.0476)\n",
      "2025-04-19 05:30:08,777 - INFO -    ↪ log_sigma_amp: 0.7203, log_sigma_ang: 0.7779\n",
      "2025-04-19 05:30:08,777 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:30:27,683 - INFO - Epoch 43/50\n",
      "2025-04-19 05:30:27,684 - INFO - 🔹 Train Loss: 6089.4410 (Amplitude: 25273.5921, Angle: 2.9548)\n",
      "2025-04-19 05:30:27,685 - INFO - 🔸 Val   Loss: 5835.2559 (Amplitude: 24466.5737, Angle: 2.8321)\n",
      "2025-04-19 05:30:27,685 - INFO - 🔻 Test  Loss: 4873.3677 (Amplitude: 20432.3197, Angle: 3.0344)\n",
      "2025-04-19 05:30:27,686 - INFO -    ↪ log_sigma_amp: 0.7405, log_sigma_ang: 0.7935\n",
      "2025-04-19 05:30:27,686 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:30:46,236 - INFO - Epoch 44/50\n",
      "2025-04-19 05:30:46,237 - INFO - 🔹 Train Loss: 5968.6265 (Amplitude: 25276.3391, Angle: 2.9656)\n",
      "2025-04-19 05:30:46,237 - INFO - 🔸 Val   Loss: 5733.9985 (Amplitude: 24531.5768, Angle: 2.8321)\n",
      "2025-04-19 05:30:46,238 - INFO - 🔻 Test  Loss: 4813.2374 (Amplitude: 20591.1500, Angle: 3.0344)\n",
      "2025-04-19 05:30:46,238 - INFO -    ↪ log_sigma_amp: 0.7606, log_sigma_ang: 0.8094\n",
      "2025-04-19 05:30:46,238 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:31:05,277 - INFO - Epoch 45/50\n",
      "2025-04-19 05:31:05,278 - INFO - 🔹 Train Loss: 5849.1952 (Amplitude: 25274.4307, Angle: 2.9613)\n",
      "2025-04-19 05:31:05,279 - INFO - 🔸 Val   Loss: 5621.5596 (Amplitude: 24538.8119, Angle: 2.8327)\n",
      "2025-04-19 05:31:05,279 - INFO - 🔻 Test  Loss: 4721.0022 (Amplitude: 20606.5770, Angle: 3.0346)\n",
      "2025-04-19 05:31:05,279 - INFO -    ↪ log_sigma_amp: 0.7808, log_sigma_ang: 0.8249\n",
      "2025-04-19 05:31:05,280 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:31:24,720 - INFO - Epoch 46/50\n",
      "2025-04-19 05:31:24,720 - INFO - 🔹 Train Loss: 5732.4256 (Amplitude: 25274.9703, Angle: 2.9603)\n",
      "2025-04-19 05:31:24,721 - INFO - 🔸 Val   Loss: 5500.6883 (Amplitude: 24502.0901, Angle: 2.8363)\n",
      "2025-04-19 05:31:24,721 - INFO - 🔻 Test  Loss: 4607.3062 (Amplitude: 20521.4078, Angle: 3.0362)\n",
      "2025-04-19 05:31:24,722 - INFO -    ↪ log_sigma_amp: 0.8010, log_sigma_ang: 0.8401\n",
      "2025-04-19 05:31:24,722 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:31:44,452 - INFO - Epoch 47/50\n",
      "2025-04-19 05:31:44,452 - INFO - 🔹 Train Loss: 5618.9372 (Amplitude: 25279.0338, Angle: 2.9577)\n",
      "2025-04-19 05:31:44,453 - INFO - 🔸 Val   Loss: 5402.5277 (Amplitude: 24554.0760, Angle: 2.8294)\n",
      "2025-04-19 05:31:44,453 - INFO - 🔻 Test  Loss: 4541.9494 (Amplitude: 20641.5734, Angle: 3.0335)\n",
      "2025-04-19 05:31:44,460 - INFO -    ↪ log_sigma_amp: 0.8211, log_sigma_ang: 0.8548\n",
      "2025-04-19 05:31:44,460 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:32:03,259 - INFO - Epoch 48/50\n",
      "2025-04-19 05:32:03,260 - INFO - 🔹 Train Loss: 5506.3113 (Amplitude: 25277.1387, Angle: 2.9546)\n",
      "2025-04-19 05:32:03,261 - INFO - 🔸 Val   Loss: 5278.2772 (Amplitude: 24478.4469, Angle: 2.8571)\n",
      "2025-04-19 05:32:03,261 - INFO - 🔻 Test  Loss: 4413.3114 (Amplitude: 20465.8140, Angle: 3.0389)\n",
      "2025-04-19 05:32:03,262 - INFO -    ↪ log_sigma_amp: 0.8413, log_sigma_ang: 0.8694\n",
      "2025-04-19 05:32:03,262 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:32:22,474 - INFO - Epoch 49/50\n",
      "2025-04-19 05:32:22,475 - INFO - 🔹 Train Loss: 5396.2968 (Amplitude: 25277.0154, Angle: 2.9677)\n",
      "2025-04-19 05:32:22,475 - INFO - 🔸 Val   Loss: 5172.4646 (Amplitude: 24476.7368, Angle: 2.8295)\n",
      "2025-04-19 05:32:22,476 - INFO - 🔻 Test  Loss: 4324.5535 (Amplitude: 20462.9876, Angle: 3.0335)\n",
      "2025-04-19 05:32:22,476 - INFO -    ↪ log_sigma_amp: 0.8615, log_sigma_ang: 0.8837\n",
      "2025-04-19 05:32:22,477 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:32:41,233 - INFO - Epoch 50/50\n",
      "2025-04-19 05:32:41,234 - INFO - 🔹 Train Loss: 5287.9539 (Amplitude: 25274.0365, Angle: 2.9584)\n",
      "2025-04-19 05:32:41,234 - INFO - 🔸 Val   Loss: 5067.5165 (Amplitude: 24468.3213, Angle: 2.8867)\n",
      "2025-04-19 05:32:41,235 - INFO - 🔻 Test  Loss: 4233.3085 (Amplitude: 20439.0207, Angle: 3.0720)\n",
      "2025-04-19 05:32:41,235 - INFO -    ↪ log_sigma_amp: 0.8817, log_sigma_ang: 0.8976\n",
      "2025-04-19 05:32:41,236 - INFO - --------------------------------------------------------------------------------\n",
      "2025-04-19 05:32:41,517 - INFO - \n",
      "✅ Best multitask model saved as 'best_multitask_model.pt' with val loss: 5067.5165\n",
      "2025-04-19 05:32:41,756 - INFO - 🧠 Best fine-tuned encoder saved as 'best_finetuned_encoder_task2.pt'\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_amp_loss, train_ang_loss = 0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_batch = y_batch.squeeze(1)\n",
    "        amp_batch, ang_batch = y_batch[:, 0], y_batch[:, 1]\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        amp_batch = amp_batch.to(device)\n",
    "        ang_batch = ang_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_amp, pred_ang = model(X_batch)\n",
    "\n",
    "        loss_amp = criterion_amplitude(pred_amp, amp_batch)\n",
    "        loss_ang = criterion_angle(pred_ang, ang_batch)\n",
    "\n",
    "        if learn_uncertainty:\n",
    "            loss = (1 / (2 * torch.exp(log_sigma_amp))) * loss_amp + \\\n",
    "                   (1 / (2 * torch.exp(log_sigma_ang))) * loss_ang + \\\n",
    "                   0.5 * (log_sigma_amp + log_sigma_ang)\n",
    "        else:\n",
    "            loss = w_amp * loss_amp + w_ang * loss_ang\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        train_amp_loss += loss_amp.item() * X_batch.size(0)\n",
    "        train_ang_loss += loss_ang.item() * X_batch.size(0)\n",
    "\n",
    "    n_train = len(train_loader.dataset)\n",
    "    avg_train_loss = train_loss / n_train\n",
    "    avg_train_amp_loss = train_amp_loss / n_train\n",
    "    avg_train_ang_loss = train_ang_loss / n_train\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_amp_loss, val_ang_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_batch = y_batch.squeeze(1)\n",
    "            amp_batch, ang_batch = y_batch[:, 0], y_batch[:, 1]\n",
    "\n",
    "            X_batch = X_batch.to(device)\n",
    "            amp_batch = amp_batch.to(device)\n",
    "            ang_batch = ang_batch.to(device)\n",
    "\n",
    "            pred_amp, pred_ang = model(X_batch)\n",
    "\n",
    "            loss_amp = criterion_amplitude(pred_amp, amp_batch)\n",
    "            loss_ang = criterion_angle(pred_ang, ang_batch)\n",
    "\n",
    "            if learn_uncertainty:\n",
    "                loss = (1 / (2 * torch.exp(log_sigma_amp))) * loss_amp + \\\n",
    "                       (1 / (2 * torch.exp(log_sigma_ang))) * loss_ang + \\\n",
    "                       0.5 * (log_sigma_amp + log_sigma_ang)\n",
    "            else:\n",
    "                loss = w_amp * loss_amp + w_ang * loss_ang\n",
    "\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            val_amp_loss += loss_amp.item() * X_batch.size(0)\n",
    "            val_ang_loss += loss_ang.item() * X_batch.size(0)\n",
    "\n",
    "    n_val = len(val_loader.dataset)\n",
    "    avg_val_loss = val_loss / n_val\n",
    "    avg_val_amp_loss = val_amp_loss / n_val\n",
    "    avg_val_ang_loss = val_ang_loss / n_val\n",
    "\n",
    "    # Test phase\n",
    "    test_loss, test_amp_loss, test_ang_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_batch = y_batch.squeeze(1)\n",
    "            amp_batch, ang_batch = y_batch[:, 0], y_batch[:, 1]\n",
    "\n",
    "            X_batch = X_batch.to(device)\n",
    "            amp_batch = amp_batch.to(device)\n",
    "            ang_batch = ang_batch.to(device)\n",
    "\n",
    "            pred_amp, pred_ang = model(X_batch)\n",
    "\n",
    "            loss_amp = criterion_amplitude(pred_amp, amp_batch)\n",
    "            loss_ang = criterion_angle(pred_ang, ang_batch)\n",
    "\n",
    "            if learn_uncertainty:\n",
    "                loss = (1 / (2 * torch.exp(log_sigma_amp))) * loss_amp + \\\n",
    "                       (1 / (2 * torch.exp(log_sigma_ang))) * loss_ang + \\\n",
    "                       0.5 * (log_sigma_amp + log_sigma_ang)\n",
    "            else:\n",
    "                loss = w_amp * loss_amp + w_ang * loss_ang\n",
    "\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "            test_amp_loss += loss_amp.item() * X_batch.size(0)\n",
    "            test_ang_loss += loss_ang.item() * X_batch.size(0)\n",
    "\n",
    "    n_test = len(test_loader.dataset)\n",
    "    avg_test_loss = test_loss / n_test\n",
    "    avg_test_amp_loss = test_amp_loss / n_test\n",
    "    avg_test_ang_loss = test_ang_loss / n_test\n",
    "\n",
    "    logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    logger.info(f\"🔹 Train Loss: {avg_train_loss:.4f} (Amplitude: {avg_train_amp_loss:.4f}, Angle: {avg_train_ang_loss:.4f})\")\n",
    "    logger.info(f\"🔸 Val   Loss: {avg_val_loss:.4f} (Amplitude: {avg_val_amp_loss:.4f}, Angle: {avg_val_ang_loss:.4f})\")\n",
    "    logger.info(f\"🔻 Test  Loss: {avg_test_loss:.4f} (Amplitude: {avg_test_amp_loss:.4f}, Angle: {avg_test_ang_loss:.4f})\")\n",
    "    if learn_uncertainty:\n",
    "        logger.info(f\"   ↪ log_sigma_amp: {log_sigma_amp.item():.4f}, log_sigma_ang: {log_sigma_ang.item():.4f}\")\n",
    "    logger.info(\"-\" * 80)\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "# Save best model and encoder state\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, \"best_multitask_model.pt\")\n",
    "    logger.info(f\"\\n✅ Best multitask model saved as 'best_multitask_model.pt' with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.encoder.encoder.state_dict(), \"best_finetuned_encoder_task2.pt\")\n",
    "    logger.info(\"🧠 Best fine-tuned encoder saved as 'best_finetuned_encoder_task2.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
