{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53124c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f5b2f",
   "metadata": {},
   "source": [
    "# Task 1: Left-Right "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "38dee3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "C:\\Users\\Aniket Konkar\\AppData\\Local\\Temp\\ipykernel_15128\\2408679250.py:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "  data = np.load('data\\LR_task_with_antisaccade_synchronised_min.npz')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30842, 2)\n",
      "Converted to\n",
      "(30842,)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data\\LR_task_with_antisaccade_synchronised_min.npz')\n",
    "print(data['labels'].shape)\n",
    "print('Converted to')\n",
    "print(data['labels'][:, 1].shape)\n",
    "\n",
    "trainX = data['EEG']\n",
    "trainY = data['labels'][:, 1]\n",
    "ids = data['labels'][:, 0] # Participant Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fedd2e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30842, 500, 129)\n",
      "(30842,)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be24320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=129, num_layers=2, num_heads=3, dim_feedforward=512):\n",
    "        super(SimpleEncoder, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True  # Input shape: [batch, seq, dim]\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Classification head: input_dim -> 1 (binary classification)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # For binary output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)  # Shape: [batch, seq_len, input_dim]\n",
    "        pooled = encoded.mean(dim=1)  # Global average pooling over time\n",
    "        return self.classifier(pooled)  # Shape: [batch, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e07b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(ids, train, val, test):\n",
    "    # proportions of train, val, test\n",
    "    assert (train+val+test == 1)\n",
    "    IDs = np.unique(ids)\n",
    "    num_ids = len(IDs)\n",
    "\n",
    "    # priority given to the test/val sets\n",
    "    test_split = math.ceil(test * num_ids)\n",
    "    val_split = math.ceil(val * num_ids)\n",
    "    train_split = num_ids - val_split - test_split\n",
    "\n",
    "    train = np.where(np.isin(ids, IDs[:train_split]))[0]\n",
    "    val = np.where(np.isin(ids, IDs[train_split:train_split+val_split]))[0]\n",
    "    test = np.where(np.isin(ids, IDs[train_split+val_split:]))[0]\n",
    "    \n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1c030861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:(21042, 500, 129) y_train.shape: (21042,)\n",
      "X_val.shape:(4980, 500, 129) y_val.shape: (4980,)\n",
      "X_test.shape:(4820, 500, 129) y_test.shape: (4820,)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "train, val, test = split(ids, 0.7, 0.15, 0.15)\n",
    "X_train, y_train = trainX[train], trainY[train]\n",
    "X_val, y_val = trainX[val], trainY[val]\n",
    "X_test, y_test = trainX[test], trainY[test]\n",
    "\n",
    "print(f\"X_train.shape:{X_train.shape} y_train.shape: {y_train.shape}\")\n",
    "print(f\"X_val.shape:{X_val.shape} y_val.shape: {y_val.shape}\")\n",
    "print(f\"X_test.shape:{X_test.shape} y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "37405250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape: (N, 1)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f3d9dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, optimizer\n",
    "model = SimpleEncoder().to(device)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 50\n",
    "best_val_acc = 0.0\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ae9c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create logs directory if not exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Set filename based on current date and time\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_filename = f\"logs/train_log_{timestamp}.log\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    filemode=\"w\",\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# Also print to console\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "logger = logging.getLogger()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "6597bae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-19 04:19:06,636 - INFO - Epoch 1/50 | Train Loss: 0.0931 | Val Acc: 97.8514 | Test Acc: 96.6390\n",
      "2025-04-19 04:19:25,140 - INFO - Epoch 2/50 | Train Loss: 0.0844 | Val Acc: 97.2691 | Test Acc: 96.9295\n",
      "2025-04-19 04:19:42,723 - INFO - Epoch 3/50 | Train Loss: 0.0831 | Val Acc: 97.1084 | Test Acc: 95.8091\n",
      "2025-04-19 04:20:00,850 - INFO - Epoch 4/50 | Train Loss: 0.0734 | Val Acc: 97.0482 | Test Acc: 96.5768\n",
      "2025-04-19 04:20:18,970 - INFO - Epoch 5/50 | Train Loss: 0.0733 | Val Acc: 97.6305 | Test Acc: 96.8880\n",
      "2025-04-19 04:20:37,117 - INFO - Epoch 6/50 | Train Loss: 0.0682 | Val Acc: 97.4096 | Test Acc: 96.7635\n",
      "2025-04-19 04:20:55,465 - INFO - Epoch 7/50 | Train Loss: 0.0710 | Val Acc: 96.8876 | Test Acc: 95.7469\n",
      "2025-04-19 04:21:13,578 - INFO - Epoch 8/50 | Train Loss: 0.0645 | Val Acc: 97.7309 | Test Acc: 97.1162\n",
      "2025-04-19 04:21:32,143 - INFO - Epoch 9/50 | Train Loss: 0.0567 | Val Acc: 97.7510 | Test Acc: 96.8050\n",
      "2025-04-19 04:21:50,279 - INFO - Epoch 10/50 | Train Loss: 0.0538 | Val Acc: 97.2691 | Test Acc: 96.5353\n",
      "2025-04-19 04:22:08,404 - INFO - Epoch 11/50 | Train Loss: 0.0524 | Val Acc: 97.3494 | Test Acc: 97.1162\n",
      "2025-04-19 04:22:26,932 - INFO - Epoch 12/50 | Train Loss: 0.0540 | Val Acc: 97.0080 | Test Acc: 95.8921\n",
      "2025-04-19 04:22:45,280 - INFO - Epoch 13/50 | Train Loss: 0.0466 | Val Acc: 97.3695 | Test Acc: 96.7012\n",
      "2025-04-19 04:23:03,765 - INFO - Epoch 14/50 | Train Loss: 0.0474 | Val Acc: 97.1084 | Test Acc: 96.7012\n",
      "2025-04-19 04:23:22,531 - INFO - Epoch 15/50 | Train Loss: 0.0494 | Val Acc: 97.3896 | Test Acc: 96.9295\n",
      "2025-04-19 04:23:40,949 - INFO - Epoch 16/50 | Train Loss: 0.0462 | Val Acc: 97.6707 | Test Acc: 97.0332\n",
      "2025-04-19 04:23:59,838 - INFO - Epoch 17/50 | Train Loss: 0.0406 | Val Acc: 97.4096 | Test Acc: 96.4938\n",
      "2025-04-19 04:24:18,297 - INFO - Epoch 18/50 | Train Loss: 0.0433 | Val Acc: 97.4699 | Test Acc: 96.5560\n",
      "2025-04-19 04:24:36,703 - INFO - Epoch 19/50 | Train Loss: 0.0400 | Val Acc: 97.3896 | Test Acc: 96.5560\n",
      "2025-04-19 04:24:55,057 - INFO - Epoch 20/50 | Train Loss: 0.0351 | Val Acc: 97.6104 | Test Acc: 97.0124\n",
      "2025-04-19 04:25:13,369 - INFO - Epoch 21/50 | Train Loss: 0.0395 | Val Acc: 97.2289 | Test Acc: 96.6390\n",
      "2025-04-19 04:25:31,669 - INFO - Epoch 22/50 | Train Loss: 0.0365 | Val Acc: 96.9478 | Test Acc: 96.5353\n",
      "2025-04-19 04:25:49,933 - INFO - Epoch 23/50 | Train Loss: 0.0328 | Val Acc: 96.8876 | Test Acc: 96.4315\n",
      "2025-04-19 04:26:08,261 - INFO - Epoch 24/50 | Train Loss: 0.0391 | Val Acc: 97.2490 | Test Acc: 96.8672\n",
      "2025-04-19 04:26:26,642 - INFO - Epoch 25/50 | Train Loss: 0.0302 | Val Acc: 97.2289 | Test Acc: 96.8672\n",
      "2025-04-19 04:26:45,207 - INFO - Epoch 26/50 | Train Loss: 0.0324 | Val Acc: 97.4498 | Test Acc: 97.0747\n",
      "2025-04-19 04:27:03,963 - INFO - Epoch 27/50 | Train Loss: 0.0307 | Val Acc: 97.2490 | Test Acc: 96.3071\n",
      "2025-04-19 04:27:22,713 - INFO - Epoch 28/50 | Train Loss: 0.0321 | Val Acc: 97.1888 | Test Acc: 96.2448\n",
      "2025-04-19 04:27:41,484 - INFO - Epoch 29/50 | Train Loss: 0.0272 | Val Acc: 96.9679 | Test Acc: 96.6390\n",
      "2025-04-19 04:28:00,165 - INFO - Epoch 30/50 | Train Loss: 0.0275 | Val Acc: 96.6867 | Test Acc: 95.9336\n",
      "2025-04-19 04:28:18,172 - INFO - Epoch 31/50 | Train Loss: 0.0303 | Val Acc: 97.0884 | Test Acc: 96.4315\n",
      "2025-04-19 04:28:35,691 - INFO - Epoch 32/50 | Train Loss: 0.0323 | Val Acc: 96.8876 | Test Acc: 95.9751\n",
      "2025-04-19 04:28:53,146 - INFO - Epoch 33/50 | Train Loss: 0.0276 | Val Acc: 96.8876 | Test Acc: 96.8465\n",
      "2025-04-19 04:29:10,742 - INFO - Epoch 34/50 | Train Loss: 0.0273 | Val Acc: 97.1888 | Test Acc: 96.5145\n",
      "2025-04-19 04:29:28,240 - INFO - Epoch 35/50 | Train Loss: 0.0255 | Val Acc: 96.5663 | Test Acc: 96.4108\n",
      "2025-04-19 04:29:47,474 - INFO - Epoch 36/50 | Train Loss: 0.0266 | Val Acc: 96.8675 | Test Acc: 96.0996\n",
      "2025-04-19 04:30:06,650 - INFO - Epoch 37/50 | Train Loss: 0.0233 | Val Acc: 96.2249 | Test Acc: 95.7884\n",
      "2025-04-19 04:30:25,794 - INFO - Epoch 38/50 | Train Loss: 0.0263 | Val Acc: 96.4659 | Test Acc: 96.1203\n",
      "2025-04-19 04:30:44,982 - INFO - Epoch 39/50 | Train Loss: 0.0273 | Val Acc: 97.3695 | Test Acc: 96.7220\n",
      "2025-04-19 04:31:04,200 - INFO - Epoch 40/50 | Train Loss: 0.0202 | Val Acc: 97.4498 | Test Acc: 96.3278\n",
      "2025-04-19 04:31:23,292 - INFO - Epoch 41/50 | Train Loss: 0.0192 | Val Acc: 96.8474 | Test Acc: 96.3278\n",
      "2025-04-19 04:31:42,384 - INFO - Epoch 42/50 | Train Loss: 0.0235 | Val Acc: 97.1687 | Test Acc: 95.9544\n",
      "2025-04-19 04:32:01,490 - INFO - Epoch 43/50 | Train Loss: 0.0216 | Val Acc: 97.2088 | Test Acc: 97.0539\n",
      "2025-04-19 04:32:20,608 - INFO - Epoch 44/50 | Train Loss: 0.0232 | Val Acc: 96.6064 | Test Acc: 96.3071\n",
      "2025-04-19 04:32:39,699 - INFO - Epoch 45/50 | Train Loss: 0.0243 | Val Acc: 97.4900 | Test Acc: 96.8880\n",
      "2025-04-19 04:32:58,793 - INFO - Epoch 46/50 | Train Loss: 0.0224 | Val Acc: 96.9880 | Test Acc: 96.5975\n",
      "2025-04-19 04:33:17,894 - INFO - Epoch 47/50 | Train Loss: 0.0183 | Val Acc: 97.1285 | Test Acc: 96.3693\n",
      "2025-04-19 04:33:37,028 - INFO - Epoch 48/50 | Train Loss: 0.0174 | Val Acc: 97.0482 | Test Acc: 95.0622\n",
      "2025-04-19 04:33:56,132 - INFO - Epoch 49/50 | Train Loss: 0.0239 | Val Acc: 97.2892 | Test Acc: 96.5560\n",
      "2025-04-19 04:34:15,239 - INFO - Epoch 50/50 | Train Loss: 0.0170 | Val Acc: 97.1285 | Test Acc: 96.3485\n",
      "2025-04-19 04:34:15,814 - INFO - \n",
      "✅ Best model saved as 'best_model.pt' with val acc: 97.8514%\n",
      "2025-04-19 04:34:15,860 - INFO - ✅ Encoder weights saved separately to 'pretrained_encoder.pt'\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * X_batch.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        val_acc = correct / total\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "    # Test evaluation each epoch\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "        test_acc = correct / total\n",
    "\n",
    "    logger.info(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Acc: {val_acc*100:.4f} | Test Acc: {test_acc*100:.4f}\")\n",
    "\n",
    "# Save best model to file\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    torch.save(best_model_state, \"best_model.pt\")\n",
    "    logger.info(f\"\\n✅ Best model saved as 'best_model.pt' with val acc: {best_val_acc*100:.4f}%\")\n",
    "\n",
    "    # Save just the encoder for reuse\n",
    "    torch.save(model.encoder.state_dict(), \"pretrained_encoder.pt\")\n",
    "    logger.info(\"✅ Encoder weights saved separately to 'pretrained_encoder.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and set to eval mode\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Move test data to device\n",
    "X_test_tensor = X_test_tensor.to(device)\n",
    "y_test_tensor = y_test_tensor.to(device)\n",
    "\n",
    "# Get predictions on full test set\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    preds = (outputs > 0.5).float()\n",
    "\n",
    "# Compute correct and wrong counts\n",
    "correct_preds = (preds == y_test_tensor).sum().item()\n",
    "total_preds = y_test_tensor.size(0)\n",
    "wrong_preds = total_preds - correct_preds\n",
    "\n",
    "# Print counts\n",
    "logger.info(f\"\\n✅ Total Correct Predictions: {correct_preds}\")\n",
    "logger.info(f\"❌ Total Wrong Predictions:   {wrong_preds}\")\n",
    "logger.info(f\"📊 Test Accuracy:             {(correct_preds / total_preds) * 100:.2f}%\")\n",
    "\n",
    "# Print predictions for first 10 test samples\n",
    "logger.info(\"\\n📊 Predictions vs Ground Truth for first 10 test samples:\\n\")\n",
    "for i in range(10):\n",
    "    pred_val = preds[i].item()\n",
    "    actual_val = y_test_tensor[i].item()\n",
    "    logger.info(f\"Sample {i+1:02d} | Predicted: {int(pred_val)} | Actual: {int(actual_val)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a7a56",
   "metadata": {},
   "source": [
    "# TASK 2: Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbaf5d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Aniket Konkar\\AppData\\Local\\Temp\\ipykernel_6988\\4130880486.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  data = np.load('data\\Direction_task_with_dots_synchronised_min_15_perc.npz')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX.shape: (2674, 500, 129)\n",
      "trainY.shape: (2674, 2)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('data\\Direction_task_with_dots_synchronised_min_15_perc.npz')\n",
    "\n",
    "trainX = data['EEG']\n",
    "trainY = data['labels'][:, 1:3]\n",
    "ids = data['labels'][:, 0] # ID\n",
    "print(f\"trainX.shape: {trainX.shape}\")\n",
    "print(f\"trainY.shape: {trainY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef51140f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:(2157, 500, 129) y_train.shape: (2157, 2)\n",
      "X_val.shape:(233, 500, 129) y_val.shape: (233, 2)\n",
      "X_test.shape:(284, 500, 129) y_test.shape: (284, 2)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "train, val, test = split(ids, 0.8, 0.1, 0.1)\n",
    "X_train, y_train = trainX[train], trainY[train]\n",
    "X_val, y_val = trainX[val], trainY[val]\n",
    "X_test, y_test = trainX[test], trainY[test]\n",
    "\n",
    "print(f\"X_train.shape:{X_train.shape} y_train.shape: {y_train.shape}\")\n",
    "print(f\"X_val.shape:{X_val.shape} y_val.shape: {y_val.shape}\")\n",
    "print(f\"X_test.shape:{X_test.shape} y_test.shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a29172be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape: (N, 2)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5466a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskRegressor(nn.Module):\n",
    "    def __init__(self, encoder, input_dim=129):\n",
    "        super(MultiTaskRegressor, self).__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # Shared head for feature extraction\n",
    "        self.shared_head = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Separate heads for amplitude and angle\n",
    "        self.amplitude_head = nn.Linear(128, 1)\n",
    "        self.angle_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder.encoder(x)  # Use encoder.encoder to get the transformer output\n",
    "        pooled = encoded.mean(dim=1)       # Same as in SimpleEncoder\n",
    "        features = self.shared_head(pooled)\n",
    "        amplitude = self.amplitude_head(features)\n",
    "        angle = self.angle_head(features)\n",
    "        return amplitude, angle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecccd8",
   "metadata": {},
   "source": [
    "# MultiTaskRegressor Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09b2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aniket Konkar\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Load encoder and its weights\n",
    "encoder = SimpleEncoder(input_dim=129).to(device)\n",
    "state_dict = torch.load(\"pretrained_encoder.pt\", map_location=device)\n",
    "encoder.encoder.load_state_dict(state_dict)\n",
    "\n",
    "# Wrap in multitask regressor\n",
    "model = MultiTaskRegressor(encoder=encoder, input_dim=129).to(device)\n",
    "\n",
    "# Angle loss with correct angle error formula (torch)\n",
    "criterion_angle = lambda pred, target: torch.mean(\n",
    "    torch.square(torch.atan2(torch.sin(target - pred), torch.cos(target - pred)))\n",
    ")\n",
    "\n",
    "# Define loss functions\n",
    "criterion_amplitude = nn.MSELoss()\n",
    "\n",
    "# Choose weighting method\n",
    "learn_uncertainty = True  # ← set to False if you want fixed weights\n",
    "\n",
    "if learn_uncertainty:\n",
    "    # Learnable log variances for adaptive weighting\n",
    "    log_sigma_amp = torch.nn.Parameter(torch.tensor(0.0, requires_grad=True, device=device))\n",
    "    log_sigma_ang = torch.nn.Parameter(torch.tensor(0.0, requires_grad=True, device=device))\n",
    "    optimizer = optim.Adam(\n",
    "        list(model.parameters()) + [log_sigma_amp, log_sigma_ang], lr=1e-4\n",
    "    )\n",
    "else:\n",
    "    # Manual weights\n",
    "    w_amp = 1.0\n",
    "    w_ang = 10000.0  # Tune based on your data\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # Define Optimizer\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training setup\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10549c5a",
   "metadata": {},
   "source": [
    "# Training for MultiTaskRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591e9100",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss, train_amp_loss, train_ang_loss = 0, 0, 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_batch = y_batch.squeeze(1)\n",
    "        amp_batch, ang_batch = y_batch[:, 0], y_batch[:, 1]\n",
    "\n",
    "        X_batch = X_batch.to(device)\n",
    "        amp_batch = amp_batch.to(device)\n",
    "        ang_batch = ang_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred_amp, pred_ang = model(X_batch)\n",
    "\n",
    "        loss_amp = criterion_amplitude(pred_amp, amp_batch)\n",
    "        loss_ang = criterion_angle(pred_ang, ang_batch)\n",
    "\n",
    "        if learn_uncertainty:\n",
    "            loss = (1 / (2 * torch.exp(log_sigma_amp))) * loss_amp + \\\n",
    "                   (1 / (2 * torch.exp(log_sigma_ang))) * loss_ang + \\\n",
    "                   0.5 * (log_sigma_amp + log_sigma_ang)\n",
    "        else:\n",
    "            loss = w_amp * loss_amp + w_ang * loss_ang\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        train_amp_loss += loss_amp.item() * X_batch.size(0)\n",
    "        train_ang_loss += loss_ang.item() * X_batch.size(0)\n",
    "\n",
    "    n_train = len(train_loader.dataset)\n",
    "    avg_train_loss = train_loss / n_train\n",
    "    avg_train_amp_loss = train_amp_loss / n_train\n",
    "    avg_train_ang_loss = train_ang_loss / n_train\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, val_amp_loss, val_ang_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            y_batch = y_batch.squeeze(1)\n",
    "            amp_batch, ang_batch = y_batch[:, 0], y_batch[:, 1]\n",
    "\n",
    "            X_batch = X_batch.to(device)\n",
    "            amp_batch = amp_batch.to(device)\n",
    "            ang_batch = ang_batch.to(device)\n",
    "\n",
    "            pred_amp, pred_ang = model(X_batch)\n",
    "\n",
    "            loss_amp = criterion_amplitude(pred_amp, amp_batch)\n",
    "            loss_ang = criterion_angle(pred_ang, ang_batch)\n",
    "\n",
    "            if learn_uncertainty:\n",
    "                loss = (1 / (2 * torch.exp(log_sigma_amp))) * loss_amp + \\\n",
    "                       (1 / (2 * torch.exp(log_sigma_ang))) * loss_ang + \\\n",
    "                       0.5 * (log_sigma_amp + log_sigma_ang)\n",
    "            else:\n",
    "                loss = w_amp * loss_amp + w_ang * loss_ang\n",
    "\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            val_amp_loss += loss_amp.item() * X_batch.size(0)\n",
    "            val_ang_loss += loss_ang.item() * X_batch.size(0)\n",
    "\n",
    "    n_val = len(val_loader.dataset)\n",
    "    avg_val_loss = val_loss / n_val\n",
    "    avg_val_amp_loss = val_amp_loss / n_val\n",
    "    avg_val_ang_loss = val_ang_loss / n_val\n",
    "\n",
    "    # Test phase\n",
    "    test_loss, test_amp_loss, test_ang_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_batch = y_batch.squeeze(1)\n",
    "            amp_batch, ang_batch = y_batch[:, 0], y_batch[:, 1]\n",
    "\n",
    "            X_batch = X_batch.to(device)\n",
    "            amp_batch = amp_batch.to(device)\n",
    "            ang_batch = ang_batch.to(device)\n",
    "\n",
    "            pred_amp, pred_ang = model(X_batch)\n",
    "\n",
    "            loss_amp = criterion_amplitude(pred_amp, amp_batch)\n",
    "            loss_ang = criterion_angle(pred_ang, ang_batch)\n",
    "\n",
    "            if learn_uncertainty:\n",
    "                loss = (1 / (2 * torch.exp(log_sigma_amp))) * loss_amp + \\\n",
    "                       (1 / (2 * torch.exp(log_sigma_ang))) * loss_ang + \\\n",
    "                       0.5 * (log_sigma_amp + log_sigma_ang)\n",
    "            else:\n",
    "                loss = w_amp * loss_amp + w_ang * loss_ang\n",
    "\n",
    "            test_loss += loss.item() * X_batch.size(0)\n",
    "            test_amp_loss += loss_amp.item() * X_batch.size(0)\n",
    "            test_ang_loss += loss_ang.item() * X_batch.size(0)\n",
    "\n",
    "    n_test = len(test_loader.dataset)\n",
    "    avg_test_loss = test_loss / n_test\n",
    "    avg_test_amp_loss = test_amp_loss / n_test\n",
    "    avg_test_ang_loss = test_ang_loss / n_test\n",
    "\n",
    "    logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    logger.info(f\"🔹 Train Loss: {avg_train_loss:.4f} (Amplitude: {avg_train_amp_loss:.4f}, Angle: {avg_train_ang_loss:.4f})\")\n",
    "    logger.info(f\"🔸 Val   Loss: {avg_val_loss:.4f} (Amplitude: {avg_val_amp_loss:.4f}, Angle: {avg_val_ang_loss:.4f})\")\n",
    "    logger.info(f\"🔻 Test  Loss: {avg_test_loss:.4f} (Amplitude: {avg_test_amp_loss:.4f}, Angle: {avg_test_ang_loss:.4f})\")\n",
    "    if learn_uncertainty:\n",
    "        logger.info(f\"   ↪ log_sigma_amp: {log_sigma_amp.item():.4f}, log_sigma_ang: {log_sigma_ang.item():.4f}\")\n",
    "    logger.info(\"-\" * 80)\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = model.state_dict()\n",
    "\n",
    "# Save best model and encoder state\n",
    "if best_model_state is not None:\n",
    "    torch.save(best_model_state, \"best_multitask_model.pt\")\n",
    "    logger.info(f\"\\n✅ Best multitask model saved as 'best_multitask_model.pt' with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    torch.save(model.encoder.encoder.state_dict(), \"best_finetuned_encoder_task2.pt\")\n",
    "    logger.info(\"🧠 Best fine-tuned encoder saved as 'best_finetuned_encoder_task2.pt'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
