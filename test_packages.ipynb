{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1809be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import ViTModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce756599",
   "metadata": {},
   "source": [
    "Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd4ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d9dbc",
   "metadata": {},
   "source": [
    "Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddf3eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGEyeNetDataset(Dataset):\n",
    "    def __init__(self, data_file,transpose = True):\n",
    "        self.data_file = data_file\n",
    "        print('loading data...')\n",
    "        with np.load(self.data_file) as f: # Load the data array\n",
    "            self.trainX = f['EEG']\n",
    "            self.trainY = f['labels']\n",
    "        print(self.trainY)\n",
    "        if transpose:\n",
    "            self.trainX = np.transpose(self.trainX, (0,2,1))[:,np.newaxis,:,:]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Read a single sample of data from the data array\n",
    "        X = torch.from_numpy(self.trainX[index]).float()\n",
    "        y = torch.from_numpy(self.trainY[index,1:3]).float()\n",
    "        # Return the tensor data\n",
    "        return (X,y,index)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Compute the number of samples in the data array\n",
    "        return len(self.trainX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7ac9fe",
   "metadata": {},
   "source": [
    "Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0599ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGViT_pretrained(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, \n",
    "            out_channels=256,\n",
    "            kernel_size=(1, 36),\n",
    "            stride=(1, 36),\n",
    "            padding=(0,2),\n",
    "            bias=False\n",
    "        )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(256, False)\n",
    "        model_name = \"google/vit-base-patch16-224\"\n",
    "        config = transformers.ViTConfig.from_pretrained(model_name)\n",
    "        config.update({'num_channels': 256})\n",
    "        config.update({'image_size': (129,14)})\n",
    "        config.update({'patch_size': (8,1)})\n",
    "\n",
    "        model = transformers.ViTForImageClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
    "        model.vit.embeddings.patch_embeddings.projection = torch.nn.Conv2d(256, 768, kernel_size=(8, 1), stride=(8, 1), padding=(0,0), groups=256)\n",
    "        model.classifier=torch.nn.Sequential(torch.nn.Linear(768,1000,bias=True),\n",
    "                                     torch.nn.Dropout(p=0.1),\n",
    "                                     torch.nn.Linear(1000,2,bias=True))\n",
    "        self.ViT = model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=self.batchnorm1(x)\n",
    "        x=self.ViT.forward(x).logits\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d8ca05",
   "metadata": {},
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66daf1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- vit.embeddings.patch_embeddings.projection.weight: found shape torch.Size([768, 3, 16, 16]) in the checkpoint and torch.Size([768, 256, 8, 1]) in the model instantiated\n",
      "- vit.embeddings.position_embeddings: found shape torch.Size([1, 197, 768]) in the checkpoint and torch.Size([1, 225, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "[[  1.  408.1 315.1]\n",
      " [  1.  640.7 519.1]\n",
      " [  1.  404.2 118.8]\n",
      " ...\n",
      " [  9.   94.2 140.7]\n",
      " [  9.  165.4 528.9]\n",
      " [  9.  152.   81.2]]\n"
     ]
    }
   ],
   "source": [
    "model = EEGViT_pretrained()\n",
    "EEGEyeNet = EEGEyeNetDataset('./data/Position_task_with_dots_synchronised_min_5_perc.npz')\n",
    "batch_size = 8\n",
    "n_epoch = 15\n",
    "learning_rate = 1e-4\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44884e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1073, 1, 129, 500)\n",
      "(1073, 3)\n"
     ]
    }
   ],
   "source": [
    "print(EEGEyeNet.trainX.shape)\n",
    "print(EEGEyeNet.trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6188ae82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1.07926917,   6.18672376,  11.43926464, ...,   4.67739071,\n",
       "           3.05745696,   1.30875489],\n",
       "        [-48.59606099, -44.37909193, -40.18544235, ...,  14.79268803,\n",
       "          14.67349957,  14.03868004],\n",
       "        [-37.82922673, -36.00171537, -34.60119476, ...,  24.99104847,\n",
       "          25.77515744,  26.30749031],\n",
       "        ...,\n",
       "        [ -1.27373337,  -0.1342227 ,   0.85801707, ...,   3.84694447,\n",
       "           3.0548663 ,   2.1669273 ],\n",
       "        [  0.80119968,   0.74955063,   0.34021102, ...,   3.65751995,\n",
       "           2.5850315 ,   1.26671835],\n",
       "        [  9.31587291,   8.48757296,   7.60863267, ...,  -4.1388763 ,\n",
       "          -3.57427646,  -3.28026298]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EEGEyeNet.trainX[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
